{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e1zkESUt9vGC",
        "eK43GG0_BUJ9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ce703e2f",
        "outputId": "578c99b0-2b08-4203-a5fb-11a02649088e"
      },
      "source": [
        "# First, install the openai-whisper library and manage dependencies for Numba.\n",
        "# This might take a moment.\n",
        "!pip uninstall -y numba numpy # Uninstall existing numba and numpy\n",
        "!pip install numpy==1.26.6 # Install a compatible numpy version\n",
        "!pip install numba # Reinstall numba to link with the new numpy\n",
        "!pip install openai-whisper"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.26.6 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.26.6\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numba\n",
            "  Downloading numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba)\n",
            "  Downloading llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting numpy<2.4,>=1.22 (from numba)\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires numba<0.62.0a0,>=0.60.0, but you have numba 0.62.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires numba<0.62.0a0,>=0.60.0, but you have numba 0.62.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llvmlite-0.45.1 numba-0.62.1 numpy-2.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "31563f999b9147bda6f8988de0dfb846"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.62.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.3.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.45.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=3f24ede6c3d3343011534f1817ea8e760c52b96bf6905fb1aeef406f2942b05b\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f93cb46",
        "outputId": "ff85905a-85aa-4667-8a6b-60dc32199d45"
      },
      "source": [
        "import whisper\n",
        "\n",
        "# Load a pre-trained Whisper model. 'tiny' is a good choice for quick testing.\n",
        "# Other options include 'base', 'small', 'medium', 'large'.\n",
        "print(\"Loading Whisper model...\")\n",
        "model = whisper.load_model(\"tiny\")\n",
        "print(\"Model loaded successfully!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 72.1M/72.1M [00:00<00:00, 181MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ccfdc7c"
      },
      "source": [
        "Before you can transcribe, you need an audio file. You can upload one to your Colab environment by clicking the folder icon on the left sidebar, then the 'Upload' button. Make sure to note its filename (e.g., `audio.mp3`).\n",
        "\n",
        "Alternatively, you can download a sample audio file from the internet, for example:\n",
        "```python\n",
        "!wget -O audio.mp3 https://www2.cs.uic.edu/~i101/samples/mp3/sample3.mp3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 逐字稿"
      ],
      "metadata": {
        "id": "e1zkESUt9vGC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "fce0a4a4",
        "outputId": "db90d246-afaf-4883-ebe9-b0755df9259d"
      },
      "source": [
        "# Replace 'your_audio_file.mp3' with the actual path to your audio file\n",
        "audio_file_path = \"ep_1746759349280_拯救雪兒.mp3\"\n",
        "\n",
        "print(f\"Transcribing audio from {audio_file_path}...\")\n",
        "result = model.transcribe(audio_file_path)\n",
        "\n",
        "# Print the transcribed text\n",
        "print(\"\\nTranscription:\")\n",
        "print(result[\"text\"])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing audio from ep_1746759349280_拯救雪兒.mp3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transcription:\n",
            "故事里的小朋友有一天 找你了路大的名字叫做 Snowy把妈倒出 都找不到她 Where did you go? Where did you go? Just find your way home Just find your way home If you lost your way If you lost your way We hope you're okay We hope you're okay We'll be saving Snowy Just find your way home We'll be saving Snowy Just find your way home You will find your way home我故事里的小朋友给我一千 她命了路她的名字叫做 Snowy慢慢到處都找不到她我绝对你不过我绝对你不过我绝对你不过我绝对你不过我绝对你不过如果你忘了现在你忘了现在我们四个新 Moy就是我起轻已经认识你了就算不赖所以很便宜只能绿生没有No waitYou and me were saved as no waitWe're everybody singing with meYou'll find your way homeThe way home is where the heart isYou'll find your way homeThe way homeCuz it will be savingYou'll find your wayYeah we'll be savingHey homeIt's where the heart isYou'll find your way homeWe'll be savingAnd we'll be savingSlowlyJust you show meJust you show meWe'll be savingSlowlyJust you show meWe'll be savingSlowlyJust you show meJust you show meWe'll be savingSlowlyJust you show meJust you show meYou'll find your way homeThe way home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 字幕"
      ],
      "metadata": {
        "id": "iZfzf2N19xXW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d3bc5b1",
        "outputId": "debf8ccb-0407-464a-f7f1-bb19fe1c4115"
      },
      "source": [
        "print(\"\\nGenerated Subtitle Segments:\")\n",
        "for segment in result[\"segments\"]:\n",
        "    start = segment[\"start\"]\n",
        "    end = segment[\"end\"]\n",
        "    text = segment[\"text\"]\n",
        "    print(f\"[{start:.2f}s -> {end:.2f}s] {text}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Subtitle Segments:\n",
            "[0.00s -> 5.00s] 故事里的小朋友\n",
            "[5.00s -> 9.00s] 有一天 找你了路\n",
            "[9.00s -> 12.00s] 大的名字叫做 Snowy\n",
            "[12.00s -> 16.00s] 把妈倒出 都找不到她\n",
            "[16.00s -> 18.00s]  Where did you go?\n",
            "[18.00s -> 20.00s]  Where did you go?\n",
            "[20.00s -> 22.00s]  Just find your way home\n",
            "[22.00s -> 24.00s]  Just find your way home\n",
            "[24.00s -> 26.00s]  If you lost your way\n",
            "[26.00s -> 28.00s]  If you lost your way\n",
            "[28.00s -> 30.00s]  We hope you're okay\n",
            "[30.00s -> 31.00s]  We hope you're okay\n",
            "[31.00s -> 33.00s]  We'll be saving Snowy\n",
            "[33.00s -> 39.00s]  Just find your way home\n",
            "[39.00s -> 41.00s]  We'll be saving Snowy\n",
            "[41.00s -> 47.00s]  Just find your way home\n",
            "[47.00s -> 51.00s]  You will find your way home\n",
            "[51.00s -> 59.00s] 我故事里的小朋友\n",
            "[59.00s -> 63.00s] 给我一千 她命了路\n",
            "[63.00s -> 66.00s] 她的名字叫做 Snowy\n",
            "[66.00s -> 71.00s] 慢慢到處都找不到她\n",
            "[71.00s -> 73.00s] 我绝对你不过\n",
            "[73.00s -> 75.00s] 我绝对你不过\n",
            "[75.00s -> 76.00s] 我绝对你不过\n",
            "[76.00s -> 77.00s] 我绝对你不过\n",
            "[77.00s -> 79.00s] 我绝对你不过\n",
            "[79.00s -> 80.80s] 如果你忘了现在\n",
            "[80.80s -> 86.20s] 你忘了现在\n",
            "[86.20s -> 88.18s] 我们四个新 Moy\n",
            "[88.18s -> 90.00s] 就是我起轻\n",
            "[90.00s -> 92.00s] 已经认识你了\n",
            "[92.00s -> 97.84s] 就算不赖\n",
            "[97.84s -> 99.00s] 所以很便宜\n",
            "[99.00s -> 100.04s] 只能绿生\n",
            "[105.04s -> 107.86s] 没有\n",
            "[107.86s -> 108.86s] No wait\n",
            "[116.42s -> 118.22s] You and me were saved as no wait\n",
            "[120.30s -> 122.30s] We're everybody singing with me\n",
            "[123.30s -> 126.14s] You'll find your way home\n",
            "[126.14s -> 130.82s] The way home is where the heart is\n",
            "[130.82s -> 134.02s] You'll find your way home\n",
            "[134.02s -> 136.18s] The way home\n",
            "[137.06s -> 138.86s] Cuz it will be saving\n",
            "[138.86s -> 140.86s] You'll find your way\n",
            "[140.86s -> 142.58s] Yeah we'll be saving\n",
            "[142.58s -> 144.18s] Hey home\n",
            "[144.18s -> 146.70s] It's where the heart is\n",
            "[146.70s -> 149.30s] You'll find your way home\n",
            "[149.30s -> 151.54s] We'll be saving\n",
            "[152.82s -> 154.26s] And we'll be saving\n",
            "[154.26s -> 155.06s] Slowly\n",
            "[155.06s -> 156.10s] Just you show me\n",
            "[156.10s -> 157.14s] Just you show me\n",
            "[157.14s -> 158.18s] We'll be saving\n",
            "[158.18s -> 158.94s] Slowly\n",
            "[158.94s -> 160.94s] Just you show me\n",
            "[160.94s -> 162.10s] We'll be saving\n",
            "[162.10s -> 162.94s] Slowly\n",
            "[162.94s -> 163.90s] Just you show me\n",
            "[163.90s -> 164.94s] Just you show me\n",
            "[164.94s -> 166.06s] We'll be saving\n",
            "[166.06s -> 167.06s] Slowly\n",
            "[167.06s -> 168.30s] Just you show me\n",
            "[168.30s -> 170.30s] Just you show me\n",
            "[170.30s -> 173.66s] You'll find your way home\n",
            "[173.66s -> 176.14s] The way home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4017d831"
      },
      "source": [
        "這些片段的資訊可以用來產生標準的字幕格式，例如 SRT 或 VTT 檔案。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 段落重點整理"
      ],
      "metadata": {
        "id": "eK43GG0_BUJ9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28ae95e"
      },
      "source": [
        "## Install Google Generative AI Library\n",
        "\n",
        "### Subtask:\n",
        "安裝 `google-generativeai` 函式庫，以便我們能夠使用 Google 的大型語言模型進行摘要。這是一個強大的模型，可以更好地理解長篇文本並提取關鍵點。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a186195b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `google-generativeai` library. I will use a `pip install` command in a code block to achieve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fa0cd3e",
        "outputId": "859daf03-dd59-4a62-bc7f-b29394eb19ff"
      },
      "source": [
        "pip install google-generativeai"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eba6267b"
      },
      "source": [
        "## Configure API Key for LLM\n",
        "\n",
        "### Subtask:\n",
        "指導使用者如何設定 Google Generative AI 的 API Key。這是在 Colab 環境中使用 Google LLM 的必要步驟。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f36a525b"
      },
      "source": [
        "## Configure API Key for LLM\n",
        "\n",
        "To use Google's Generative AI models, you need an API key. Please follow these steps to obtain and configure your key:\n",
        "\n",
        "1.  **Obtain your API Key**: Go to the [Google AI Studio](https://makersuite.google.com/keys) or Google Cloud Console to create or retrieve your API key. If you don't have one, you can create it there.\n",
        "\n",
        "2.  **Keep your API Key secure**: Do not share your API key publicly or embed it directly in your code in a production environment. For this Colab notebook, we will temporarily place it in the code for demonstration purposes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc255aaa"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the user has been instructed on how to obtain the API key, I will provide a code block demonstrating how to import the `google.generativeai` library and configure the API key using a placeholder for their actual key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dacfd01",
        "outputId": "51b6513e-c86f-4dc5-9c88-85ee90031749"
      },
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# IMPORTANT: Replace \"YOUR_API_KEY\" with your actual Google Generative AI API key\n",
        "genai.configure(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "print(\"Google Generative AI API key configured.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI API key configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6dae334"
      },
      "source": [
        "## Load Google Generative AI Model\n",
        "\n",
        "### Subtask:\n",
        "載入 Google 的大型語言模型，例如 `gemini-pro`，用於生成關鍵重點摘要。這是一個更適合處理複雜摘要任務的模型。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "125bd373"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to load the `gemini-pro` model. I will use the `genai.GenerativeModel` function to load the model and store it in a variable as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e759152d",
        "outputId": "f557ebae-f13e-45f6-daaa-92132b5f124c"
      },
      "source": [
        "print(\"Loading Google Generative AI model...\")\n",
        "model_llm = genai.GenerativeModel('gemini-pro')\n",
        "print(\"Model loaded successfully!\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Google Generative AI model...\n",
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9c6fcf"
      },
      "source": [
        "## Generate Key Point Summaries with Timestamps\n",
        "\n",
        "### Subtask:\n",
        "整合完整的逐字稿內容，並使用載入的 Google Generative AI 模型生成多個重點摘要。模型將被提示去提取關鍵資訊，並嘗試將這些關鍵點與原始字幕段落的時間起訖點進行關聯。最終輸出會以 `[起始時間s -> 結束時間s] 摘要內容` 的格式呈現，其中時間範圍將盡可能準確地反映摘要內容在原始音頻中的位置。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a866c4"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to format the transcription segments into a single string to feed into the LLM. This involves iterating through the `result[\"segments\"]` and constructing a string in the specified format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8c34fd4",
        "outputId": "7978fb0d-8ebe-4e60-a2c3-bbf5138f2ee3"
      },
      "source": [
        "transcription_text = \"\"\n",
        "for segment in result[\"segments\"]:\n",
        "    start = segment[\"start\"]\n",
        "    end = segment[\"end\"]\n",
        "    text = segment[\"text\"]\n",
        "    transcription_text += f\"[{start:.2f}s -> {end:.2f}s] {text}\\n\"\n",
        "\n",
        "print(\"Formatted Transcription (first 500 characters):\")\n",
        "print(transcription_text[:500])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted Transcription (first 500 characters):\n",
            "[0.00s -> 5.00s] 故事里的小朋友\n",
            "[5.00s -> 9.00s] 有一天 找你了路\n",
            "[9.00s -> 12.00s] 大的名字叫做 Snowy\n",
            "[12.00s -> 16.00s] 把妈倒出 都找不到她\n",
            "[16.00s -> 18.00s]  Where did you go?\n",
            "[18.00s -> 20.00s]  Where did you go?\n",
            "[20.00s -> 22.00s]  Just find your way home\n",
            "[22.00s -> 24.00s]  Just find your way home\n",
            "[24.00s -> 26.00s]  If you lost your way\n",
            "[26.00s -> 28.00s]  If you lost your way\n",
            "[28.00s -> 30.00s]  We hope you're okay\n",
            "[30.00s -> 31.00s]  We hope you're okay\n",
            "[31.00s -> 33.00s]  We'll be saving Snowy\n",
            "[33.00s -> 39.00s]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acce51cd"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the transcription text is formatted, I will construct a prompt for the `model_llm` to summarize the content with timestamps, then call the model and print its response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "005a8bbb",
        "outputId": "5a7df8a9-5225-4851-df45-29fc113c05df"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# Get the API key from the user directly if it's not set as an environment variable\n",
        "# This method is safer than hardcoding it directly in the notebook.\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google Generative AI API Key: \")\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "print(\"Google Generative AI API key configured.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google Generative AI API Key: ··········\n",
            "Google Generative AI API key configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75f8df54"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the API key has now been correctly configured, I will re-execute the code to generate key point summaries using the `model_llm` and the formatted transcription.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "50b1802d",
        "outputId": "b601e4c6-c78d-4a2b-8341-651996de4b50"
      },
      "source": [
        "print(\"Listing available generative models...\")\n",
        "for m in genai.list_models():\n",
        "  if \"generateContent\" in m.supported_generation_methods:\n",
        "    print(m.name)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing available generative models...\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a641391"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error indicated that 'gemini-pro' was not found. Based on the list of available models, I will re-initialize the `model_llm` with 'models/gemini-pro-latest' which is likely the correct model to use for general content generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9f8509b",
        "outputId": "97b05f20-fa9c-4b39-c05d-b018594251cb"
      },
      "source": [
        "print(\"Loading Google Generative AI model...\")\n",
        "model_llm = genai.GenerativeModel('models/gemini-pro-latest')\n",
        "print(\"Model loaded successfully!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Google Generative AI model...\n",
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db587e74"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `model_llm` has been re-initialized with the correct model name, I will re-execute the code to generate key point summaries using the `model_llm` and the formatted transcription.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "e872ad08",
        "outputId": "716d2afc-54f5-4523-c5fa-d31ebe65aa2e"
      },
      "source": [
        "prompt = f\"\"\"請根據以下音頻逐字稿，提取主要關鍵點或重要段落，並為每個關鍵點提供大致的起始時間和結束時間。時間格式為 `[起始時間s -> 結束時間s]`，摘要內容。\n",
        "\n",
        "逐字稿內容：\n",
        "{transcription_text}\n",
        "\n",
        "請以以下格式輸出：\n",
        "[起始時間s -> 結束時間s] 摘要內容\n",
        "[起始時間s -> 結束時間s] 摘要內容\n",
        "...\"\"\"\n",
        "\n",
        "print(\"Generating key point summaries...\")\n",
        "response = model_llm.generate_content(prompt)\n",
        "\n",
        "print(\"\\nKey Point Summaries with Timestamps:\")\n",
        "print(response.text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating key point summaries...\n",
            "\n",
            "Key Point Summaries with Timestamps:\n",
            "好的，這是在分析您的逐字稿後，提取出的主要關鍵點和摘要：\n",
            "\n",
            "[0.00s -> 16.00s] 故事開頭介紹了一個名叫 Snowy 的小朋友迷路了，大家都找不到她。\n",
            "\n",
            "[16.00s -> 47.00s] 歌曲的副歌部分，用英文表達對 Snowy 的擔憂與期盼，不斷呼喊希望她能找到回家的路，並表明會去拯救她。\n",
            "\n",
            "[51.00s -> 71.00s] 再次以中文重述 Snowy 走失的情節，加強故事的背景。\n",
            "\n",
            "[71.00s -> 92.00s] 一段情緒強烈的段落，表達了複雜的情感，並提及關於遺忘與記憶的主題。\n",
            "\n",
            "[123.30s -> 151.54s] 歌曲進入高潮，強調「家是心之所在 (The way home is where the heart is)」，堅信 Snowy 最終會找到回家的路。\n",
            "\n",
            "[151.54s -> 176.14s] 歌曲結尾，以反覆呼喊「We'll be saving Snowy」作結，再次強化拯救與盼其歸來的主題。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e047861c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "將由大型語言模型生成的每個段落重點摘要，連同其對應的時間起訖點，以使用者要求的格式輸出。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2deda53b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The main question was: How to output key point summaries generated by a large language model, along with their corresponding start and end times, in a user-requested format?\n",
        "\n",
        "The process successfully demonstrated how to format the original transcription with timestamps and then use a configured Google Generative AI model (`models/gemini-pro-latest`) to generate summaries, addressing several configuration and model-loading challenges along the way.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `google-generativeai` library was already installed (version 0.8.5) and its dependencies were satisfied, indicating it was ready for use.\n",
        "*   Initial attempts to use the LLM failed due to an invalid API key, highlighting the critical need for proper API key configuration before model instantiation.\n",
        "*   After API key configuration, the LLM calls still failed because the `genai.GenerativeModel` instance was \"stale,\" meaning it was initialized before the API key was correctly set. Re-initializing the model after configuring the API key was necessary.\n",
        "*   Another failure occurred because the model name `'gemini-pro'` was not found or supported for the `generateContent` method. Listing available models via `genai.list_models()` revealed that `'models/gemini-pro-latest'` was the correct and supported model name.\n",
        "*   Upon correctly configuring the API key and re-initializing the model with `'models/gemini-pro-latest'`, the Google Generative AI model successfully generated key point summaries from the provided transcription text. The summaries were outputted in the requested format: `[起始時間s -> 結束時間s] 摘要內容`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   When working with generative AI APIs, always ensure the API key is configured *before* initializing the model instance. If the key is updated or configured dynamically, existing model instances must be re-initialized.\n",
        "*   If a \"model not found\" or similar error occurs, use the API's provided methods (e.g., `genai.list_models()`) to verify available model names and their supported functionalities, as model names and capabilities can change.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 錯字處理"
      ],
      "metadata": {
        "id": "Is3Dw4u3-Ox0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "typo_correction_prompt = f\"\"\"請檢查以下音頻逐字稿，並修正其中的錯別字。特別是，請將所有簡體中文字轉換為繁體中文字。輸出時，只返回修正後的文本，不需要任何額外的說明或格式。\\n\\n逐字稿內容：\\n{transcription_text}\\n\"\"\"\n",
        "\n",
        "print(\"Generating corrected and traditional Chinese text...\")\n",
        "corrected_response = model_llm.generate_content(typo_correction_prompt)\n",
        "\n",
        "print(\"\\nCorrected and Traditional Chinese Transcription:\")\n",
        "print(corrected_response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3UzStEXY-TUF",
        "outputId": "9366f26b-dcfe-4601-cf8f-b9ccf9b8a970"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating corrected and traditional Chinese text...\n",
            "\n",
            "Corrected and Traditional Chinese Transcription:\n",
            "[0.00s -> 5.00s] 故事裡的小朋友\n",
            "[5.00s -> 9.00s] 有一天 迷了路\n",
            "[9.00s -> 12.00s] 她的名字叫做 Snowy\n",
            "[12.00s -> 16.00s] 爸爸媽媽到處都找不到她\n",
            "[16.00s -> 18.00s]  Where did you go?\n",
            "[18.00s -> 20.00s]  Where did you go?\n",
            "[20.00s -> 22.00s]  Just find your way home\n",
            "[22.00s -> 24.00s]  Just find your way home\n",
            "[24.00s -> 26.00s]  If you lost your way\n",
            "[26.00s -> 28.00s]  If you lost your way\n",
            "[28.00s -> 30.00s]  We hope you're okay\n",
            "[30.00s -> 31.00s]  We hope you're okay\n",
            "[31.00s -> 33.00s]  We'll be saving Snowy\n",
            "[33.00s -> 39.00s]  Just find your way home\n",
            "[39.00s -> 41.00s]  We'll be saving Snowy\n",
            "[41.00s -> 47.00s]  Just find your way home\n",
            "[47.00s -> 51.00s]  You will find your way home\n",
            "[51.00s -> 59.00s] 我故事裡的小朋友\n",
            "[59.00s -> 63.00s] 有一天 她迷了路\n",
            "[63.00s -> 66.00s] 她的名字叫做 Snowy\n",
            "[66.00s -> 71.00s] 媽媽到處都找不到她\n",
            "[71.00s -> 73.00s] 我絕對你不過\n",
            "[73.00s -> 75.00s] 我絕對你不過\n",
            "[75.00s -> 76.00s] 我絕對你不過\n",
            "[76.00s -> 77.00s] 我絕對你不過\n",
            "[77.00s -> 79.00s] 我絕對你不過\n",
            "[79.00s -> 80.80s] 如果你忘了現在\n",
            "[80.80s -> 86.20s] 你忘了現在\n",
            "[86.20s -> 88.18s] 我們四個新 Moy\n",
            "[88.18s -> 90.00s] 就是我起初\n",
            "[90.00s -> 92.00s] 已經認識你了\n",
            "[92.00s -> 97.84s] 就算不賴\n",
            "[97.84s -> 99.00s] 所以很便宜\n",
            "[99.00s -> 100.04s] 只能錄聲\n",
            "[105.04s -> 107.86s] 沒有\n",
            "[107.86s -> 108.86s] No wait\n",
            "[116.42s -> 118.22s] You and me were saved as no wait\n",
            "[120.30s -> 122.30s] We're everybody singing with me\n",
            "[123.30s -> 126.14s] You'll find your way home\n",
            "[126.14s -> 130.82s] The way home is where the heart is\n",
            "[130.82s -> 134.02s] You'll find your way home\n",
            "[134.02s -> 136.18s] The way home\n",
            "[137.06s -> 138.86s] Cuz it will be saving\n",
            "[138.86s -> 140.86s] You'll find your way\n",
            "[140.86s -> 142.58s] Yeah we'll be saving\n",
            "[142.58s -> 144.18s] Hey home\n",
            "[144.18s -> 146.70s] It's where the heart is\n",
            "[146.70s -> 149.30s] You'll find your way home\n",
            "[149.30s -> 151.54s] We'll be saving\n",
            "[152.82s -> 154.26s] And we'll be saving\n",
            "[154.26s -> 155.06s] Snowy\n",
            "[155.06s -> 156.10s] Just you show me\n",
            "[156.10s -> 157.14s] Just you show me\n",
            "[157.14s -> 158.18s] We'll be saving\n",
            "[158.18s -> 158.94s] Snowy\n",
            "[158.94s -> 160.94s] Just you show me\n",
            "[160.94s -> 162.10s] We'll be saving\n",
            "[162.10s -> 162.94s] Snowy\n",
            "[162.94s -> 163.90s] Just you show me\n",
            "[163.90s -> 164.94s] Just you show me\n",
            "[164.94s -> 166.06s] We'll be saving\n",
            "[166.06s -> 167.06s] Snowy\n",
            "[167.06s -> 168.30s] Just you show me\n",
            "[168.30s -> 170.30s] Just you show me\n",
            "[170.30s -> 173.66s] You'll find your way home\n",
            "[173.66s -> 176.14s] The way home\n"
          ]
        }
      ]
    }
  ]
}